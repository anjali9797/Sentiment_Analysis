# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lwQTljG1i-YjZw3lkXOmJyxmjDOywyZt

Importing the necessary packages
"""

import pandas as pd
import csv
import numpy as np
import matplotlib
import random
from sklearn.metrics import accuracy_score
import os
from sklearn.dummy import DummyClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import copy
import re
import nltk
from nltk.corpus import stopwords
from nltk.corpus import words
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

from google.colab import drive
drive.mount('/content/drive')

#reading the data 
df=pd.read_csv("/content/drive/MyDrive/train.csv",encoding="latin1")
#separating out the text and the labels
#the 0th column contains the sentiment corresponding to the column and the 5th column contains the text of the tweet
df=df.sample(frac=1)

data_x=df.iloc[:90500,5]
data_y=df.iloc[:90500,0]



#converting the labels
data_y=list(data_y)
#visualising the data we have
num_neutral=0
num_positive=0
num_negative=0
for i in data_y:
    if(i==2):
        num_neutral=num_neutral+1
    elif(i==4):
        num_positive=num_positive+1
    else:
        num_negative=num_negative+1



x = ['Negative Tweets','Neutral Tweets','Positive Tweets']
fig = plt.figure(figsize = (10, 8)) 
plt.bar(x[0], num_negative, color ='pink', width = 0.2) 
#plt.bar(x[1], num_neutral, color ='pink', width = 0.2) 
plt.bar(x[2], num_positive, color ='green', width = 0.2) 
plt.xlabel("") 
plt.ylabel("No of tweets") 
plt.title("Comparing the Number of positive and negative tweets") 
plt.show()



#Analysing the train and test data and separating accordingly



#downloading stopwords
import nltk
nltk.download('stopwords')
stops=set(stopwords.words('english'))

nltk.download('words')
setofwords = set(words.words())

"""## Function to clean the data for Further Processing """

def clean_data(data):
    datas=data.values.tolist()
    for i in range(len(data)):
        s=datas[i]
        #converting to lower case for convinience
        s=s.lower().split()
        #now let us remove the stop words
        tes=list()
        for w in s:
            if "https" in w:
                continue
            if w not in stops:
                if w.isnumeric()==False:
                    if w[0]!='@' and w[0]!="#":
                        if len(w)>=2:
                            tes.append(w)
        tes=" ".join(tes)
        #replacing nums and other characters
        tes=re.sub(r'https?://[A-Za-z0-9./]+','',tes)
        tes=re.sub(r"[0-9^,!.\/''+-=]"," ",tes)
        tes=re.sub(r","," ",tes)
        tes=re.sub(r"!"," ",tes)
        tes=re.sub(r"#","",tes)
        tes=re.sub(r"$","",tes)
        tes=re.sub(r"%","",tes)
        tes=re.sub(r"^","",tes)
        tes=re.sub(r"&","",tes)
        tes=re.sub(r"\*","",tes)
        tes=re.sub(r"\("," ",tes)
        tes=re.sub(r"\)"," ",tes)
        tes=re.sub(r"_"," ",tes)
        tes=re.sub(r":","",tes)
        tes=re.sub(r"-"," ",tes)
        datas[i]=tes
    return datas

data_X=clean_data(data_x)
print(len(data_X))



def get_label(datas):
    #datas=datas.values.tolist()
    ans=list()
    for i in range(len(datas)):
        if datas[i] == 4:
            ans.append(0)
        else:
            ans.append(1)
    return datas

data_Y=get_label(data_y)

cv = CountVectorizer(max_features = 2000)
data_x = cv.fit_transform(data_X).toarray()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_x, data_Y, test_size = 0.01, random_state=0)

classifier_np = GaussianNB()
classifier_np.fit(X_train, y_train)
print(len(X_train))

y_pred = classifier_np.predict(X_test)
from sklearn.metrics import confusion_matrix, accuracy_score
acc1=accuracy_score(y_test,y_pred)

print(acc1)
import seaborn as sns
conf=confusion_matrix(y_test,y_pred)
sns.heatmap(conf, annot=True, cmap='Blues', fmt=".1f")
from sklearn.metrics import f1_score
f11=f1_score(y_test,y_pred,average='micro')
print(f11)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
acc2=accuracy_score(y_test,y_pred)
print(acc2)
conf=confusion_matrix(y_test,y_pred)
sns.heatmap(conf, annot=True, cmap='Blues', fmt=".1f")
from sklearn.metrics import f1_score
f12=f1_score(y_test,y_pred,average='macro')
print(f12)

classifier_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier_dt.fit(X_train, y_train)
pred=classifier_dt.predict(X_test)
acc3=accuracy_score(y_test,pred)
print(acc3)
conf=confusion_matrix(y_test,y_pred)
sns.heatmap(conf, annot=True, cmap='Blues', fmt=".1f")



classifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier_rf.fit(X_train, y_train)

pred=classifier_rf.predict(X_test)
acc4=accuracy_score(y_test,pred)
print(acc4)
conf=confusion_matrix(y_test,y_pred)
sns.heatmap(conf, annot=True, cmap='Blues', fmt=".1f")

from sklearn.ensemble import StackingClassifier
level0 = list()
level0.append(('rff', RandomForestClassifier(n_estimators=10)))
level0.append(('gnb',GaussianNB()))
level0.append(('l',LogisticRegression(max_iter=2000)))
model = StackingClassifier(estimators=level0, final_estimator=LogisticRegression(max_iter=2000), cv=4)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
acc5=accuracy_score(y_test,y_pred)
print(acc5)
conf=confusion_matrix(y_test,y_pred)
sns.heatmap(conf, annot=True, cmap='Blues', fmt=".1f")

print(acc2)

model.fit(data_x,data_Y)

dff=pd.read_csv("/content/drive/MyDrive/Covishield_tweets.csv",encoding="latin1")

test_x=dff.iloc[:,4]
test_x=clean_data(test_x)
cvv = CountVectorizer(max_features = 2000)
test_x = cvv.fit_transform(test_x).toarray()

y_pred = model.predict(test_x)

print(y_pred)
#converting the labels
data_y=list(y_pred)
#visualising the data we have
num_neutral=0
num_positive=0
num_negative=0
for i in data_y:
    if(i==2):
        num_neutral=num_neutral+1
    elif(i==4):
        num_positive=num_positive+1
    else:
        num_negative=num_negative+1

x = ['Negative Tweets','Neutral Tweets','Positive Tweets']
fig = plt.figure(figsize = (10, 8)) 
plt.bar(x[0], num_negative, color ='pink', width = 0.2) 
#plt.bar(x[1], num_neutral, color ='pink', width = 0.2) 
plt.bar(x[2], num_positive, color ='green', width = 0.2) 
plt.xlabel("") 
plt.ylabel("No of tweets") 
plt.title("Comparing the Number of positive and negative tweets for covishield") 
plt.show()

test_x=dff.iloc[:,4]
test_x=clean_data(test_x)

positive=list()
negative=list()